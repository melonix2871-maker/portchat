<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>PortChat â€“ Local AI Chat</title>
  <script src="https://cdn.jsdelivr.net/npm/marked@4.3.0/marked.min.js"></script>
  <style>
    :root {
      --bg: #0d1117;
      --text: #c9d1d9;
      --user-bg: #238636;
      --bot-bg: #30363d;
      --input-bg: #161b22;
      --border: #30363d;
      --accent: #58a6ff;
    }
    body {
      margin: 0;
      font-family: system-ui, sans-serif;
      background: var(--bg);
      color: var(--text);
      height: 100vh;
      display: flex;
      flex-direction: column;
    }
    header {
      padding: 12px 16px;
      background: #010409;
      border-bottom: 1px solid var(--border);
      display: flex;
      justify-content: space-between;
      align-items: center;
      gap: 12px;
      flex-wrap: wrap;
    }
    h1 { margin: 0; font-size: 1.4rem; }
    #model-select {
      padding: 8px 12px;
      background: var(--input-bg);
      color: white;
      border: 1px solid var(--border);
      border-radius: 6px;
      font-size: 0.95rem;
    }
    #status {
      padding: 10px;
      background: #161b22;
      color: #8b949e;
      text-align: center;
      font-size: 0.9rem;
    }
    #chat-container {
      flex: 1;
      overflow-y: auto;
      padding: 16px;
      display: flex;
      flex-direction: column;
      gap: 14px;
    }
    .message {
      max-width: 86%;
      padding: 12px 16px;
      border-radius: 16px;
      line-height: 1.48;
    }
    .user {
      align-self: flex-end;
      background: var(--user-bg);
      border-bottom-right-radius: 4px;
    }
    .assistant {
      align-self: flex-start;
      background: var(--bot-bg);
      border-bottom-left-radius: 4px;
    }
    #input-area {
      padding: 12px 16px;
      background: #010409;
      border-top: 1px solid var(--border);
      display: flex;
      gap: 10px;
    }
    #user-input {
      flex: 1;
      padding: 12px 18px;
      border: 1px solid var(--border);
      border-radius: 24px;
      background: var(--input-bg);
      color: white;
      font-size: 1rem;
      outline: none;
      resize: none;
      min-height: 44px;
      max-height: 160px;
    }
    #send-btn {
      padding: 0 22px;
      background: var(--accent);
      color: white;
      border: none;
      border-radius: 24px;
      font-weight: 600;
      cursor: pointer;
    }
    #send-btn:disabled {
      background: #444;
      cursor: not-allowed;
    }
    pre, code {
      background: #161b22;
      padding: 10px 14px;
      border-radius: 8px;
      overflow-x: auto;
      font-family: 'Consolas', monospace;
      font-size: 0.92rem;
    }
  </style>
</head>
<body>

  <header>
    <h1>PortChat â€“ Local AI</h1>
     <select id="model-select">
  <option value="TinyLlama-1.1B-Chat-v1.0-q4f32_1-MLC">TinyLlama 1.1B â€“ fastest & most compatible</option>
  <option value="RedPajama-INCITE-Chat-3B-v1-q4f32_1-MLC">RedPajama 3B â€“ very stable</option>
  <option value="Phi-2-q4f32_1-MLC">Phi-2 â€“ good quality & reliable</option>
</select>
    <label style="display:flex; align-items:center; gap:8px; font-size:0.9rem;">
      <input type="checkbox" id="low-vram"/>
      Low VRAM mode
    </label>
  </header>

  <div id="status">Select a model to beginâ€¦ (first load takes several minutes)</div>

  <div id="chat-container"></div>

  <div id="input-area">
    <textarea id="user-input" rows="1" placeholder="Ask anything..."></textarea>
    <button id="send-btn" disabled>Send</button>
  </div>

  <script type="module">
    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    //   Correct modern import (2025â€“2026 versions)
    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    //import { CreateMLCEngine } from "https://cdn.jsdelivr.net/npm/@mlc-ai/web-llm@0.2.48/+esm";
    //import { CreateMLCEngine } from "https://cdn.jsdelivr.net/npm/@mlc-ai/web-llm/+esm";

    // Pinned CDN import for stability (JSDelivr ESM)
    import { MLCEngine } from "https://cdn.jsdelivr.net/npm/@mlc-ai/web-llm@0.2.80/+esm";

    let engine = null;
    let messages = [
      { role: "system", content: "You are a helpful, concise and truthful assistant." }
    ];

    const statusEl        = document.getElementById("status");
    const chatContainer   = document.getElementById("chat-container");
    const userInput       = document.getElementById("user-input");
    const sendBtn         = document.getElementById("send-btn");
    const modelSelect     = document.getElementById("model-select");
    const lowVram         = document.getElementById("low-vram");

    // Auto-resize input
    userInput.addEventListener("input", () => {
      userInput.style.height = "auto";
      userInput.style.height = userInput.scrollHeight + "px";
    });

    // Enter = send (without shift)
    userInput.addEventListener("keydown", e => {
      if (e.key === "Enter" && !e.shiftKey) {
        e.preventDefault();
        sendMessage();
      }
    });

    sendBtn.addEventListener("click", sendMessage);

    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    //   Load / reload model when selection changes
    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    async function loadModel() {
      const modelId = modelSelect.value;
      if (!modelId) return;

      statusEl.textContent = `Loading ${modelSelect.options[modelSelect.selectedIndex].text} â€¦`;
      sendBtn.disabled = true;
      userInput.disabled = true;

      try {
        // Create engine, then load model with constrained chat opts to reduce VRAM usage
        engine = new MLCEngine({
          initProgressCallback: report => {
            statusEl.textContent = report.text || "Downloading model filesâ€¦";
          }
        });
        const chatOpts = lowVram.checked ? {
          context_window_size: 1024,
          sliding_window_size: -1,
          attention_sink_size: 128,
          temperature: 0.7
        } : {
          context_window_size: 2048,
          sliding_window_size: -1,
          attention_sink_size: 160,
          temperature: 0.7
        };
        await engine.reload(modelId, chatOpts);

        statusEl.textContent = `Ready â€“ using ${modelSelect.options[modelSelect.selectedIndex].text} ${lowVram.checked ? "(Low VRAM)" : "(Fast Mode)"}`;
        sendBtn.disabled = false;
        userInput.disabled = false;
        userInput.focus();

        addMessage("assistant", `Loaded **${modelSelect.options[modelSelect.selectedIndex].text}** successfully with reduced context for stability.\nAsk me anything ðŸš€`);
      } catch (err) {
        statusEl.textContent = "Error: " + err.message;
        console.error(err);
      }
    }

    modelSelect.addEventListener("change", loadModel);

    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    //   Send message & stream response
    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    async function sendMessage() {
      const text = userInput.value.trim();
      if (!text || !engine) return;

      addMessage("user", text);
      userInput.value = "";
      userInput.style.height = "auto";
      sendBtn.disabled = true;
      userInput.disabled = true;

      messages.push({ role: "user", content: text });

      const replyDiv = addMessage("assistant", "");
      let fullText = "";

      try {
        const stream = await engine.chat.completions.create({
          messages,
          stream: true,
          temperature: 0.7,
          max_gen_len: lowVram.checked ? 256 : 384
        });

        for await (const chunk of stream) {
          const delta = chunk.choices?.[0]?.delta?.content ?? "";
          fullText += delta;
          replyDiv.innerHTML = marked.parse(fullText);
          chatContainer.scrollTop = chatContainer.scrollHeight;
        }

        messages.push({ role: "assistant", content: fullText });
      } catch (err) {
        const msg = String(err?.message || err);
        // Handle GPU OOM / device lost gracefully by auto-downshifting
        if (msg.includes("GPUOutOfMemoryError") || msg.includes("Device was lost")) {
          statusEl.textContent = "GPU memory issue detected; switching to TinyLlama with smaller contextâ€¦";
          try {
            // Switch to smallest model and tighter limits
            modelSelect.value = "TinyLlama-1.1B-Chat-v1.0-q4f32_1-MLC";
            engine = new MLCEngine({
              initProgressCallback: report => {
                statusEl.textContent = report.text || "Adjusting modelâ€¦";
              }
            });
            await engine.reload(modelSelect.value, {
              context_window_size: 1024,
              sliding_window_size: -1,
              attention_sink_size: 128,
              temperature: 0.7
            });
            statusEl.textContent = "Recovered â€“ TinyLlama ready";
            sendBtn.disabled = false;
            userInput.disabled = false;
            replyDiv.textContent = "Recovery complete. Please resend your question.";
            addMessage("assistant", "Tip: If GPU errors repeat, toggle Low VRAM mode or use a Remote API endpoint for stable performance.");
          } catch (e2) {
            replyDiv.textContent = "Recovery failed: " + (e2?.message || String(e2));
          }
        } else {
          replyDiv.textContent = "Error: " + msg;
          console.error(err);
        }
      }

      sendBtn.disabled = false;
      userInput.disabled = false;
      userInput.focus();
    }

    function addMessage(role, content) {
      const div = document.createElement("div");
      div.className = `message ${role}`;
      div.innerHTML = marked.parse(content);
      chatContainer.appendChild(div);
      chatContainer.scrollTop = chatContainer.scrollHeight;
      return div;
    }

    // Start with the default (first) model (TinyLlama) for fastest startup
    loadModel();
  </script>
</body>
</html>
